{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ae0641c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-04T18:54:40.446473Z",
     "iopub.status.busy": "2024-08-04T18:54:40.446113Z",
     "iopub.status.idle": "2024-08-04T18:55:57.339741Z",
     "shell.execute_reply": "2024-08-04T18:55:57.338482Z"
    },
    "papermill": {
     "duration": 76.901876,
     "end_time": "2024-08-04T18:55:57.342354",
     "exception": false,
     "start_time": "2024-08-04T18:54:40.440478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U peft --no-index --find-links ../input/common-pip/\n",
    "!pip install -q -U accelerate --no-index --find-links ../input/common-pip/\n",
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/common-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/common-pip/\n",
    "!pip install -q -U sentencepiece --no-index --find-links ../input/common-pip/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4aa7bef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T18:55:57.351774Z",
     "iopub.status.busy": "2024-08-04T18:55:57.351479Z",
     "iopub.status.idle": "2024-08-04T18:55:57.362563Z",
     "shell.execute_reply": "2024-08-04T18:55:57.361771Z"
    },
    "papermill": {
     "duration": 0.018191,
     "end_time": "2024-08-04T18:55:57.364631",
     "exception": false,
     "start_time": "2024-08-04T18:55:57.346440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_LLM_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_LLM_inference.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import yaml\n",
    "from ast import literal_eval\n",
    "from dataclasses import dataclass\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    Gemma2ForSequenceClassification,\n",
    "    GemmaTokenizerFast,\n",
    "    LlamaForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "from scipy.special import expit, softmax\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, TaskType, PeftModelForSequenceClassification\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "import gc\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "def process_prompt(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "def prepare_tokenized(data, tokenizer, max_length=2048, spread_max_length=False):\n",
    "    if isinstance(tokenizer, GemmaTokenizerFast):\n",
    "        prompt = [\"<prompt>: \" + p for p in data['prompt']]\n",
    "        response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in data['response_a']]\n",
    "        response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in data['response_b']]\n",
    "    else:\n",
    "        prompt = [\"User prompt: \" + p for p in data['prompt']]\n",
    "        response_a = [\"\\n\\nModel A: \\n\" + r_a for r_a in data['response_a']]\n",
    "        response_b = [\"\\n\\nModel B: \\n\" + r_b for r_b in data['response_b']]\n",
    "\n",
    "    if spread_max_length:\n",
    "        prompt = tokenizer(prompt, max_length=prompt_len, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=response_len, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=response_len, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]\n",
    "    else:\n",
    "        prompt_len = max_length//4\n",
    "        prompt = [p[:prompt_len] + \"<cont>\" if len(p) > prompt_len else p for p in prompt]\n",
    "\n",
    "        response_len = [(max_length - len(p))//2 for p in prompt]\n",
    "        response_a = [r[:rl] + \"<cont>\" if len(r)>rl else r for r, rl in zip(response_a, response_len)]\n",
    "        response_b = [r[:rl] + \"<cont>\" if len(r)>rl else r for r, rl in zip(response_b, response_len)]\n",
    "\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "    data.update({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "    return data\n",
    "\n",
    "def main(args):\n",
    "    test_df = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/test.csv\")\n",
    "    \n",
    "    accelerator.print(f'Test csv shape: {test_df.shape}')\n",
    "    \n",
    "    test_df['prompt'] = test_df['prompt'].fillna(\"\").apply(process_prompt)\n",
    "    test_df['response_a'] = test_df['response_a'].fillna(\"\").apply(process_prompt)\n",
    "    test_df['response_b'] = test_df['response_b'].fillna(\"\").apply(process_prompt)\n",
    "    test_df['text'] = test_df['prompt'].fillna(\"\") + test_df['response_a'].fillna(\"\") + test_df['response_b'].fillna(\"\")\n",
    "    test_df['prompt_len'] = test_df['text'].apply(lambda x: len(x))\n",
    "    test_df.sort_values(by='prompt_len', inplace=True, ascending=False)\n",
    "    \n",
    "    ## Load tokenizer\n",
    "    if \"gemma\" in args.base_model_path:\n",
    "        tokenizer = GemmaTokenizerFast.from_pretrained(args.base_model_path)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.base_model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    \n",
    "    test_df.reset_index(inplace=True, drop=True)\n",
    "    test_ds = Dataset.from_list(test_df[['prompt', 'response_a', 'response_b']].to_dict('records'), split=\"test\")\n",
    "    \n",
    "    test_tokenized_ds = test_ds.map(prepare_tokenized, batched=True, \n",
    "                                    fn_kwargs={\"max_length\": args.max_length, \"tokenizer\": tokenizer, \"spread_max_length\": False},\n",
    "                                    remove_columns=test_ds.column_names)\n",
    "\n",
    "    \n",
    "    \n",
    "    ## Load Model\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_8bit_use_double_quant=False,\n",
    "        bnb_8bit_compute_dtype=torch.float16,\n",
    "        llm_int8_skip_modules=[\"score\", \"correctness\", \"classifier\", \"model_pred1\", \"model_pred2\"]\n",
    "    )\n",
    "    \n",
    "    if \"gemma\" in args.base_model_path:\n",
    "        base_model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "            args.base_model_path,\n",
    "            quantization_config=bnb_config,\n",
    "            torch_dtype=torch.float16, \n",
    "            trust_remote_code=True,\n",
    "            problem_type=\"single_label_classification\",\n",
    "            num_labels=3\n",
    "        )\n",
    "    else:\n",
    "        base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "            args.base_model_path,\n",
    "            quantization_config=bnb_config,\n",
    "            torch_dtype=torch.float16, \n",
    "            trust_remote_code=True,\n",
    "            problem_type=\"single_label_classification\",\n",
    "            num_labels=3\n",
    "        )\n",
    "    base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    base_model.config.use_cache = False\n",
    "    # base_model.enable_input_require_grads()\n",
    "    \n",
    "    base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "    model = PeftModelForSequenceClassification.from_pretrained(base_model, args.lora_path, is_trainable=False)\n",
    "    # model = accelerator.prepare(model)\n",
    "    \n",
    "    ## Trainer Setup\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "    training_args = TrainingArguments(output_dir=\"tmp\", \n",
    "                                  per_device_eval_batch_size=args.batch_size,\n",
    "                                  remove_unused_columns=False,\n",
    "                                  batch_eval_metrics=True,\n",
    "                                  report_to=None\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    \n",
    "    ## predictor\n",
    "    pred_output = trainer.predict(test_tokenized_ds, ignore_keys=[\"past_key_values\"])\n",
    "    logits = pred_output.predictions.astype(float).reshape(-1, 3)\n",
    "\n",
    "    # probs = softmax(logits, axis=1)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        \"id\": test_df['id'].values,\n",
    "        \"logits_0\": logits[:, 0],\n",
    "        \"logits_1\": logits[:, 1],\n",
    "        \"logits_2\": logits[:, 2],\n",
    "    })\n",
    "    \n",
    "    results.to_csv(args.save_dir, index=False)\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument('--base_model_path', type=str, required=True)\n",
    "    ap.add_argument('--lora_path', type=str, required=True)\n",
    "    ap.add_argument('--max_length', type=int, required=True)\n",
    "    ap.add_argument('--batch_size', type=int, required=True)\n",
    "    ap.add_argument('--save_dir', type=str, default=\"submission.csv\")\n",
    "    args = ap.parse_args()\n",
    "    \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b1ac791",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T18:55:57.372837Z",
     "iopub.status.busy": "2024-08-04T18:55:57.372577Z",
     "iopub.status.idle": "2024-08-04T18:59:55.768188Z",
     "shell.execute_reply": "2024-08-04T18:59:55.767235Z"
    },
    "papermill": {
     "duration": 238.402471,
     "end_time": "2024-08-04T18:59:55.770663",
     "exception": false,
     "start_time": "2024-08-04T18:55:57.368192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-04 18:56:10.749459: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-08-04 18:56:10.749459: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-08-04 18:56:10.749530: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-08-04 18:56:10.749575: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-08-04 18:56:10.860303: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2024-08-04 18:56:10.860299: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Test csv shape: (3, 4)\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 151.81 examples/s]\r\n",
      "Unused kwargs: ['bnb_8bit_use_double_quant', 'bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\r\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 119.77 examples/s]\r\n",
      "Unused kwargs: ['bnb_8bit_use_double_quant', 'bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\r\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\r\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:167: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:167: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\r\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [03:20<00:00, 66.93s/it]\r\n",
      "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/gemma2-9b-it-base/ and are newly initialized: ['score.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [03:20<00:00, 66.94s/it]\r\n",
      "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/gemma2-9b-it-base/ and are newly initialized: ['score.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\r\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\r\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 56.78it/s]\r\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 run_LLM_inference.py \\\n",
    "--lora_path \"/kaggle/input/gemma2-8b-it-lora-2k-fold2-ft/\" \\\n",
    "--base_model_path \"/kaggle/input/gemma2-9b-it-base/\" \\\n",
    "--max_length 1536 \\\n",
    "--batch_size 2 \\\n",
    "--save_dir \"out1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b63e4b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T18:59:55.782583Z",
     "iopub.status.busy": "2024-08-04T18:59:55.782287Z",
     "iopub.status.idle": "2024-08-04T19:03:07.658382Z",
     "shell.execute_reply": "2024-08-04T19:03:07.657417Z"
    },
    "papermill": {
     "duration": 191.884627,
     "end_time": "2024-08-04T19:03:07.660732",
     "exception": false,
     "start_time": "2024-08-04T18:59:55.776105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-04 19:00:03.989110: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-08-04 19:00:03.989172: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-08-04 19:00:03.990619: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2024-08-04 19:00:04.005657: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-08-04 19:00:04.005705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-08-04 19:00:04.007113: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Test csv shape: (3, 4)\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 391.10 examples/s]\r\n",
      "Unused kwargs: ['bnb_8bit_use_double_quant', 'bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\r\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\r\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:167: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 359.74 examples/s]\r\n",
      "Unused kwargs: ['bnb_8bit_use_double_quant', 'bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\r\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\r\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:167: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [02:48<00:00, 84.08s/it]\r\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/llama3-8b-it-base/ and are newly initialized: ['score.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [02:48<00:00, 84.16s/it]\r\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/llama3-8b-it-base/ and are newly initialized: ['score.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\r\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\r\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 30.46it/s]\r\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 run_LLM_inference.py \\\n",
    "--lora_path \"/kaggle/input/llama3-8b-it-lora-2k-fold2-ft/\" \\\n",
    "--base_model_path \"/kaggle/input/llama3-8b-it-base/\" \\\n",
    "--max_length 2048 \\\n",
    "--batch_size 8 \\\n",
    "--save_dir \"out2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd5909a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T19:03:07.675170Z",
     "iopub.status.busy": "2024-08-04T19:03:07.674841Z",
     "iopub.status.idle": "2024-08-04T19:03:08.174799Z",
     "shell.execute_reply": "2024-08-04T19:03:08.173385Z"
    },
    "papermill": {
     "duration": 0.511417,
     "end_time": "2024-08-04T19:03:08.178801",
     "exception": false,
     "start_time": "2024-08-04T19:03:07.667384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import expit, softmax\n",
    "\n",
    "df1 = pd.read_csv(\"out1.csv\")\n",
    "df2 = pd.read_csv(\"out2.csv\")\n",
    "\n",
    "best_w = 0.55\n",
    "best_temp = 0.9\n",
    "logits1 = df1[['logits_0', 'logits_1', 'logits_2']].values\n",
    "logits2 = df2[['logits_0', 'logits_1', 'logits_2']].values\n",
    "logits = best_w*logits1.copy() + (1-best_w)*logits2.copy()\n",
    "probs = softmax(logits/best_temp, 1).astype(np.float64)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"id\": df1['id'].values,\n",
    "    \"winner_model_a\": probs[:, 0].astype(np.float64),\n",
    "    \"winner_model_b\": probs[:, 1].astype(np.float64),\n",
    "    \"winner_tie\": probs[:, 2].astype(np.float64),\n",
    "})\n",
    "\n",
    "results_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c6e7139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T19:03:08.200664Z",
     "iopub.status.busy": "2024-08-04T19:03:08.200344Z",
     "iopub.status.idle": "2024-08-04T19:03:08.204540Z",
     "shell.execute_reply": "2024-08-04T19:03:08.203566Z"
    },
    "papermill": {
     "duration": 0.013999,
     "end_time": "2024-08-04T19:03:08.206532",
     "exception": false,
     "start_time": "2024-08-04T19:03:08.192533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results_df = pd.DataFrame({\n",
    "#     \"id\": df1['id'].values,\n",
    "#     \"winner_model_a\": probs[:, 0].astype(np.float64),\n",
    "#     \"winner_model_b\": probs[:, 1].astype(np.float64),\n",
    "#     \"winner_tie\": probs[:, 2].astype(np.float64),\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b02584f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T19:03:08.221696Z",
     "iopub.status.busy": "2024-08-04T19:03:08.221107Z",
     "iopub.status.idle": "2024-08-04T19:03:08.225016Z",
     "shell.execute_reply": "2024-08-04T19:03:08.224055Z"
    },
    "papermill": {
     "duration": 0.0138,
     "end_time": "2024-08-04T19:03:08.227239",
     "exception": false,
     "start_time": "2024-08-04T19:03:08.213439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"\\nData type of column 'B':\", results_df['winner_tie'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e05c07b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T19:03:08.242007Z",
     "iopub.status.busy": "2024-08-04T19:03:08.241686Z",
     "iopub.status.idle": "2024-08-04T19:03:08.245513Z",
     "shell.execute_reply": "2024-08-04T19:03:08.244704Z"
    },
    "papermill": {
     "duration": 0.013327,
     "end_time": "2024-08-04T19:03:08.247437",
     "exception": false,
     "start_time": "2024-08-04T19:03:08.234110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# pd.read_csv(\"submission.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4fe7b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T19:03:08.262124Z",
     "iopub.status.busy": "2024-08-04T19:03:08.261832Z",
     "iopub.status.idle": "2024-08-04T19:03:08.265453Z",
     "shell.execute_reply": "2024-08-04T19:03:08.264623Z"
    },
    "papermill": {
     "duration": 0.013082,
     "end_time": "2024-08-04T19:03:08.267349",
     "exception": false,
     "start_time": "2024-08-04T19:03:08.254267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# sample = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "615011dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T19:03:08.281663Z",
     "iopub.status.busy": "2024-08-04T19:03:08.281424Z",
     "iopub.status.idle": "2024-08-04T19:03:08.284799Z",
     "shell.execute_reply": "2024-08-04T19:03:08.284016Z"
    },
    "papermill": {
     "duration": 0.012288,
     "end_time": "2024-08-04T19:03:08.286588",
     "exception": false,
     "start_time": "2024-08-04T19:03:08.274300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# type(results_df['winner_model_a'][0])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5488224,
     "sourceId": 9094433,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5488235,
     "sourceId": 9094447,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5489443,
     "sourceId": 9096079,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5489457,
     "sourceId": 9096095,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 188676450,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 191031200,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 191032280,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 510.850043,
   "end_time": "2024-08-04T19:03:08.611152",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-04T18:54:37.761109",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
